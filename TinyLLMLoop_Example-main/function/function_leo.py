import pdfplumber
import fitz  # PyMuPDF
import cv2
import numpy as np
import os
import re
import json
from pathlib import Path #ç”¨äºæ‹¼æ¥mdæ–‡ä»¶
from PyPDF2 import PdfReader

def page_ranges_from_list(pages):
    """æŠŠé¡µç åˆ—è¡¨åˆå¹¶æˆè¿ç»­åŒºé—´"""
    if not pages:
        return []
    pages = sorted(set(pages))
    ranges = []
    start = prev = pages[0]
    for p in pages[1:]:
        if p == prev + 1:
            prev = p
        else:
            ranges.append((start, prev))
            start = prev = p
    ranges.append((start, prev))
    return ranges

def is_toc_like(text):
    """åˆ¤æ–­é¡µé¢æ˜¯å¦åƒç›®å½•é¡µ"""
    if not text or not text.strip():
        return False
    if "ç›®å½•" in text:
        return True
    page_end_re = re.compile(r'(\.{2,}|\s{2,})\d{1,4}\s*$')
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    matches = sum(1 for ln in lines if page_end_re.search(ln))
    return matches >= 2

def chinese_char_ratio(text):
    """è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹"""
    if not text:
        return 0.0
    total = len(text)
    chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
    return chinese / total if total > 0 else 0.0

# def extract_pdf_info(pdf_path):
#     doc = fitz.open(pdf_path)
#     page_texts = [doc[p].get_text("text") or "" for p in range(doc.page_count)]
#     total_pages = doc.page_count
#
#     # æå– PDF å†…ç½® TOC
#     toc = doc.get_toc()
#     level1 = [(title.strip(), int(page)) for lv, title, page in toc if lv == 1]
#     level1.sort(key=lambda x: x[1])
#
#     # æ£€æµ‹æ‘˜è¦é¡µ
#     abstract_re = re.compile(r'\babstract\b', re.I)
#     keywords_re = re.compile(r'\bKeywords\b|\bå…³é”®è¯\b', re.I)
#     ch_pages, en_pages = [], []
#     for i, txt in enumerate(page_texts, start=1):
#         if "æ‘˜è¦" in txt and not is_toc_like(txt) and chinese_char_ratio(txt) > 0.02:
#             ch_pages.append(i)
#         if abstract_re.search(txt) and not is_toc_like(txt):
#             en_pages.append(i)
#     ch_ranges = page_ranges_from_list(ch_pages)
#
#     # è‹±æ–‡æ‘˜è¦èŒƒå›´
#     en_range = None
#     if en_pages:
#         start = min(en_pages)
#         end = start
#         for p in range(start, total_pages + 1):
#             txt = page_texts[p - 1]
#             if p != start and (is_toc_like(txt) or keywords_re.search(txt) or re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+ç« ', txt)):
#                 end = p - 1
#                 break
#             end = p
#         en_range = (start, end)
#
#     # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£å¼ç« èŠ‚èµ·å§‹é¡µ
#     main_level1 = [(t, p) for (t, p) in level1 if not re.fullmatch(r'(?i)abstract', t) and 'æ‘˜è¦' not in t]
#     first_main_chapter_start = main_level1[0][1] if main_level1 else None
#
#     # ç›®å½•é¡µèŒƒå›´ = æ‘˜è¦ç»“æŸåˆ°ç¬¬ä¸€ç« ä¹‹å‰
#     last_abs_page = 0
#     if ch_ranges:
#         last_abs_page = max(e for (_, e) in ch_ranges)
#     if en_range:
#         last_abs_page = max(last_abs_page, en_range[1])
#     directory = None
#     if last_abs_page and first_main_chapter_start and first_main_chapter_start > last_abs_page + 1:
#         directory = (last_abs_page + 1, first_main_chapter_start - 1)
#
#     # æ±‡æ€»ç»“æœ
#     sections = []
#     for s, e in ch_ranges:
#         sections.append({"title": "æ‘˜è¦", "start_page": s, "end_page": e})
#     if en_range:
#         sections.append({"title": "Abstract", "start_page": en_range[0], "end_page": en_range[1]})
#     if directory:
#         sections.append({"title": "ç›®å½•", "start_page": directory[0], "end_page": directory[1]})
#     for title, page in main_level1:
#         sections.append({"title": title, "start_page": page})
#
#     # æŒ‰é¡µç æ’åº
#     sections.sort(key=lambda x: x["start_page"])
#
#     # å¡«å…… end_page
#     for i, sec in enumerate(sections):
#         if "end_page" in sec:
#             continue
#         if i + 1 < len(sections):
#             sec["end_page"] = sections[i + 1]["start_page"] - 1
#         else:
#             sec["end_page"] = total_pages
#         if sec["end_page"] < sec["start_page"]:
#             sec["end_page"] = sec["start_page"]
#
#     # åˆå¹¶åŒåæ¡ç›®ï¼Œä¿®å¤åŒºé—´
#     def merge_sections(secs, total):
#         by_title = {}
#         for s in secs:
#             t = s["title"]
#             a, b = int(s["start_page"]), int(s["end_page"])
#             if b < a:
#                 b = a
#             if t in by_title:
#                 by_title[t]["start_page"] = min(by_title[t]["start_page"], a)
#                 by_title[t]["end_page"] = max(by_title[t]["end_page"], b)
#             else:
#                 by_title[t] = {"title": t, "start_page": a, "end_page": b}
#         merged = list(by_title.values())
#         merged.sort(key=lambda x: x["start_page"])
#         skip = {"æ‘˜è¦", "Abstract", "ç›®å½•"}
#         for i in range(len(merged) - 1):
#             if merged[i]["title"] in skip:
#                 continue
#             if merged[i]["end_page"] >= merged[i + 1]["start_page"]:
#                 merged[i]["end_page"] = merged[i + 1]["start_page"] - 1
#                 if merged[i]["end_page"] < merged[i]["start_page"]:
#                     merged[i]["end_page"] = merged[i]["start_page"]
#         if merged:
#             merged[-1]["end_page"] = min(merged[-1]["end_page"], total)
#         return merged
#
#     return {"chapters": merge_sections(sections, total_pages)}
def extract_pdf_info(pdf_path):
    doc = fitz.open(pdf_path)
    page_texts = [doc[p].get_text("text") or "" for p in range(doc.page_count)]
    total_pages = doc.page_count

    # æå– PDF å†…ç½® TOC
    toc = doc.get_toc()
    level1 = [(title.strip(), int(page)) for lv, title, page in toc if lv == 1]
    level1.sort(key=lambda x: x[1])

    # æ£€æµ‹æ‘˜è¦é¡µ
    abstract_re = re.compile(r'\babstract\b', re.I)
    keywords_re = re.compile(r'\bKeywords\b|\bå…³é”®è¯\b', re.I)
    ch_pages, en_pages = [], []
    for i, txt in enumerate(page_texts, start=1):
        if "æ‘˜è¦" in txt and not is_toc_like(txt) and chinese_char_ratio(txt) > 0.02:
            ch_pages.append(i)
        if abstract_re.search(txt) and not is_toc_like(txt):
            en_pages.append(i)
    ch_ranges = page_ranges_from_list(ch_pages)

    # è‹±æ–‡æ‘˜è¦èŒƒå›´
    en_range = None
    if en_pages:
        start = min(en_pages)
        end = start
        for p in range(start, total_pages + 1):
            txt = page_texts[p - 1]
            if p != start and (is_toc_like(txt) or keywords_re.search(txt) or re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+ç« ', txt)):
                end = p - 1
                break
            end = p
        en_range = (start, end)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£å¼ç« èŠ‚èµ·å§‹é¡µ
    main_level1 = [(t, p) for (t, p) in level1 if not re.fullmatch(r'(?i)abstract', t) and 'æ‘˜è¦' not in t]
    first_main_chapter_start = main_level1[0][1] if main_level1 else None

    # ç›®å½•é¡µèŒƒå›´ = æ‘˜è¦ç»“æŸåˆ°ç¬¬ä¸€ç« ä¹‹å‰
    last_abs_page = 0
    if ch_ranges:
        last_abs_page = max(e for (_, e) in ch_ranges)
    if en_range:
        last_abs_page = max(last_abs_page, en_range[1])
    directory = None
    if last_abs_page and first_main_chapter_start and first_main_chapter_start > last_abs_page + 1:
        directory = (last_abs_page + 1, first_main_chapter_start - 1)

    # æ±‡æ€»ç»“æœ
    sections = []
    # ğŸ‘‰ æ•´åˆä¸­è‹±æ–‡æ‘˜è¦
    if ch_ranges or en_range:
        s = min([r[0] for r in ch_ranges] + ([en_range[0]] if en_range else []))
        e = max([r[1] for r in ch_ranges] + ([en_range[1]] if en_range else []))
        sections.append({"title": "æ‘˜è¦", "start_page": s, "end_page": e})

    if directory:
        sections.append({"title": "ç›®å½•", "start_page": directory[0], "end_page": directory[1]})
    for title, page in main_level1:
        sections.append({"title": title, "start_page": page})

    # æŒ‰é¡µç æ’åº
    sections.sort(key=lambda x: x["start_page"])

    # å¡«å…… end_page
    for i, sec in enumerate(sections):
        if "end_page" in sec:
            continue
        if i + 1 < len(sections):
            sec["end_page"] = sections[i + 1]["start_page"] - 1
        else:
            sec["end_page"] = total_pages
        if sec["end_page"] < sec["start_page"]:
            sec["end_page"] = sec["start_page"]

    # åˆå¹¶åŒåæ¡ç›®ï¼Œä¿®å¤åŒºé—´
    def merge_sections(secs, total):
        by_title = {}
        for s in secs:
            t = s["title"]
            a, b = int(s["start_page"]), int(s["end_page"])
            if b < a:
                b = a
            if t in by_title:
                by_title[t]["start_page"] = min(by_title[t]["start_page"], a)
                by_title[t]["end_page"] = max(by_title[t]["end_page"], b)
            else:
                by_title[t] = {"title": t, "start_page": a, "end_page": b}
        merged = list(by_title.values())
        merged.sort(key=lambda x: x["start_page"])
        skip = {"æ‘˜è¦", "ç›®å½•"}
        for i in range(len(merged) - 1):
            if merged[i]["title"] in skip:
                continue
            if merged[i]["end_page"] >= merged[i + 1]["start_page"]:
                merged[i]["end_page"] = merged[i + 1]["start_page"] - 1
                if merged[i]["end_page"] < merged[i]["start_page"]:
                    merged[i]["end_page"] = merged[i]["start_page"]
        if merged:
            merged[-1]["end_page"] = min(merged[-1]["end_page"], total)
        return merged

    return {"chapters": merge_sections(sections, total_pages)}





#jsonè½¬markdown
def json_to_md(json_path, md_path="outline_converted.md"):
    """
    å°† JSON è½¬æ¢å› Markdown æ ¼å¼
    """
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    lines = []
    for chap in data:
        lines.append(f"# {chap['chapter']}\n")
        for sec in chap["sections"]:
            lines.append(f"## {sec['section_number']} {sec['title']}\n")
            for point in sec["writing_points"]:
                lines.append(f"- \u3000\u3000{point}\n")
            lines.append("\n")

    with open(md_path, "w", encoding="utf-8") as f:
        f.writelines(lines)

    return md_path

#æ›´æ–°jsonæ–‡ä»¶å†…å®¹
# def update_writing_point(json_path, section_number, index, new_text):
#     """
#     ä¿®æ”¹æŒ‡å®š section_number ä¸‹æŸä¸ªå†™ä½œè¦ç‚¹
#     :param json_path: JSON æ–‡ä»¶è·¯å¾„
#     :param section_number: ç« èŠ‚å· (ä¾‹å¦‚ "1.1")
#     :param index: å†™ä½œè¦ç‚¹ç´¢å¼• (ä» 0 å¼€å§‹)
#     :param new_text: æ›¿æ¢åçš„æ–‡æœ¬
#     """
#     # 1. è¯»å– JSON
#     with open(json_path, "r", encoding="utf-8") as f:
#         data = json.load(f)
#
#     # 2. æŸ¥æ‰¾ç›®æ ‡ section
#     for chapter in data:
#         for section in chapter["sections"]:
#             if section["section_number"] == section_number:
#                 if 0 <= index < len(section["writing_points"]):
#                     old_text = section["writing_points"][index]
#                     section["writing_points"][index] = new_text
#                     print(f"âœ… å·²ä¿®æ”¹: [{section_number}] ç¬¬{index}æ¡\n æ—§: {old_text}\n æ–°: {new_text}")
#                 else:
#                     print(f"âŒ ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ (å…± {len(section['writing_points'])} æ¡)")
#
#     # 3. ä¿å­˜å›æ–‡ä»¶
#     with open(json_path, "w", encoding="utf-8") as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)
#æ›´æ–°jsonæ–‡ä»¶å†…å®¹2
def update_writing_point(json_path, section_number, index, new_text):
    """
    ä¿®æ”¹æŒ‡å®š section_number ä¸‹æŸä¸ªå†™ä½œè¦ç‚¹
    :param json_path: JSON æ–‡ä»¶è·¯å¾„
    :param section_number: ç« èŠ‚å· (ä¾‹å¦‚ "1.1")
    :param index: å†™ä½œè¦ç‚¹ç´¢å¼• (ä» 0 å¼€å§‹)
    :param new_text: æ›¿æ¢åçš„æ–‡æœ¬
    """
    # 1. è¯»å– JSON
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # 2. æŸ¥æ‰¾ç›®æ ‡ section
    for chapter in data:
        for section in chapter["sections"]:
            if section["section_number"] == section_number:
                if 0 <= index < len(section["writing_points"]):
                    point = section["writing_points"][index]

                    # æƒ…å†µ 1ï¼šåŸæœ¬æ˜¯å­—ç¬¦ä¸²
                    if isinstance(point, str):
                        old_text = point
                        section["writing_points"][index] = new_text

                    # æƒ…å†µ 2ï¼šæ˜¯å­—å…¸ï¼Œä¿®æ”¹å…¶ä¸­çš„ text å­—æ®µ
                    elif isinstance(point, dict) and "text" in point:
                        old_text = point["text"]
                        section["writing_points"][index]["text"] = new_text

                    else:
                        print(f"âš ï¸ ç¬¬ {index} æ¡å†™ä½œè¦ç‚¹æ ¼å¼å¼‚å¸¸: {point}")
                        return

                    print(f"âœ… å·²ä¿®æ”¹: [{section_number}] ç¬¬{index}æ¡\n æ—§: {old_text}\n æ–°: {new_text}")
                else:
                    print(f"âŒ ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ (å…± {len(section['writing_points'])} æ¡)")

    # 3. ä¿å­˜å›æ–‡ä»¶
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

#outline.jsonæ ¼å¼è½¬æ¢,å»æ‰"images"é”®å€¼å¯¹
def convert_outline(in_path, out_path):
    # è¯»å–åŸå§‹ JSON
    with open(in_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    def simplify_points(points):
        result = []
        for p in points:
            if isinstance(p, dict) and "text" in p:
                result.append(p["text"])
            elif isinstance(p, str):
                result.append(p)
        return result

    # éå†ç« èŠ‚ï¼Œæ›¿æ¢ writing_points
    for chapter in data:
        for section in chapter.get("sections", []):
            section["writing_points"] = simplify_points(section.get("writing_points", []))

    # ä¿å­˜åˆ°æ–°æ–‡ä»¶
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f"âœ… è½¬æ¢å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {out_path}")


#markdownè½¬json version1
def md_to_json(md_path, json_path="outline.json"):
    """
    å°† Markdown æçº²è½¬æˆ JSON æ ¼å¼
    """
    with open(md_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    data = []
    current_chapter = None
    current_section = None

    for line in lines:
        line = line.strip()

        # åŒ¹é…ç« èŠ‚å’Œå°èŠ‚
        chapter_match = re.match(r"^#\s*(.+)", line)
        section_match = re.match(r"^##+\s+(\d+(\.\d+)+)\s+(.+)", line)

        if chapter_match and not line.startswith("##"):
            current_chapter = chapter_match.group(1)
            data.append({
                "chapter": current_chapter,
                "sections": []
            })

        elif section_match:
            current_section = {
                "section_number": section_match.group(1),
                "title": section_match.group(3),
                "writing_points": []
            }
            data[-1]["sections"].append(current_section)

        # åŒ¹é…å†™ä½œè¦ç‚¹
        elif line.startswith("- å†™ä½œè¦ç‚¹"):
            point = line.replace("- å†™ä½œè¦ç‚¹ï¼š", "").strip()
            if current_section:
                current_section["writing_points"].append(point)

    # ä¿å­˜ä¸º JSON
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    return data

#markdownè½¬json version2
def md_to_json2(md_path, json_path="outline.json"):
    """
    å°† Markdown æçº²è½¬æˆ JSON æ ¼å¼
    - å†™ä½œè¦ç‚¹å­˜å…¥ "writing_points"
    - å›¾ç‰‡å­˜å…¥ "images"
    """
    with open(md_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    data = []
    current_chapter = None
    current_section = None

    for line in lines:
        line = line.strip()

        # åŒ¹é…ç« èŠ‚
        chapter_match = re.match(r"^#\s*(.+)", line)
        section_match = re.match(r"^##+\s+(\d+(\.\d+)+)\s+(.+)", line)

        if chapter_match and not line.startswith("##"):
            current_chapter = chapter_match.group(1)
            data.append({
                "chapter": current_chapter,
                "sections": []
            })

        elif section_match:
            current_section = {
                "section_number": section_match.group(1),
                "title": section_match.group(3),
                "writing_points": [],
                "images": []
            }
            data[-1]["sections"].append(current_section)

        # åŒ¹é…å†™ä½œè¦ç‚¹
        elif line.startswith("- å†™ä½œè¦ç‚¹"):
            point = line.replace("- å†™ä½œè¦ç‚¹ï¼š", "").strip()
            if current_section:
                current_section["writing_points"].append(point)

        # åŒ¹é… Markdown å›¾ç‰‡
        elif line.startswith("![]") or line.startswith("!["):
            img_match = re.match(r"!\[(.*?)\]\((.*?)\)", line)
            if img_match and current_section:
                current_section["images"].append({
                    "legend": img_match.group(1),
                    "path": img_match.group(2)
                })

    # ä¿å­˜ä¸º JSON
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    return data

def md_to_json3(md_path, json_path="outline.json"):
    """
    å°† Markdown æçº²è½¬æˆ JSON æ ¼å¼
    - æ¯ä¸ªå†™ä½œè¦ç‚¹ç»‘å®šå›¾ç‰‡
    """
    with open(md_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    data = []
    current_chapter = None
    current_section = None
    current_point = None  # ç”¨æ¥å­˜å‚¨æ­£åœ¨å¤„ç†çš„å†™ä½œè¦ç‚¹

    for line in lines:
        line = line.strip()

        # åŒ¹é…ç« èŠ‚
        chapter_match = re.match(r"^#\s*(.+)", line)
        section_match = re.match(r"^##+\s+(\d+(\.\d+)+)\s+(.+)", line)

        if chapter_match and not line.startswith("##"):
            current_chapter = chapter_match.group(1)
            data.append({"chapter": current_chapter, "sections": []})

        elif section_match:
            current_section = {
                "section_number": section_match.group(1),
                "title": section_match.group(3),
                "writing_points": []
            }
            data[-1]["sections"].append(current_section)

        # åŒ¹é…å†™ä½œè¦ç‚¹
        elif line.startswith("- å†™ä½œè¦ç‚¹"):
            point_text = line.replace("- å†™ä½œè¦ç‚¹ï¼š", "").replace("- å†™ä½œè¦ç‚¹:", "").strip()
            current_point = {"text": point_text, "images": []}
            if current_section:
                current_section["writing_points"].append(current_point)

        # åŒ¹é…å›¾ç‰‡ï¼ˆå¦‚æœåœ¨å†™ä½œè¦ç‚¹ä¹‹åï¼Œå°±ç»‘åˆ°è¯¥å†™ä½œè¦ç‚¹ï¼‰
        elif line.startswith("![]") or line.startswith("!["):
            img_match = re.match(r"!\[(.*?)\]\((.*?)\)", line)
            if img_match:
                img_obj = {"legend": img_match.group(1), "path": img_match.group(2)}
                if current_point:  # ç»‘å®šåˆ°æœ€è¿‘çš„å†™ä½œè¦ç‚¹
                    current_point["images"].append(img_obj)
                elif current_section:  # å¦‚æœæ²¡æœ‰å†™ä½œè¦ç‚¹ï¼Œå°±æ”¾åˆ° section é‡Œ
                    current_section.setdefault("images", []).append(img_obj)

    # ä¿å­˜ä¸º JSON
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    return data
# #æå–pdfå†…æ‘˜è¦ã€ç›®å½•çš„é¡µç ä»¥åŠpdfæ€»å­—æ•°,è¿”å›jsonæ–‡ä»¶
# def extract_pdf_info(pdf_path, output_json="pdf_info.json"):
#     result = {
#         "abstract_page": None,
#         "toc_page": None,
#         "total_pages": 0
#     }
#
#     # è·å–æ€»é¡µæ•°
#     reader = PdfReader(pdf_path)
#     result["total_pages"] = len(reader.pages)
#
#     # éå† PDF æ¯ä¸€é¡µ
#     with pdfplumber.open(pdf_path) as pdf:
#         for i, page in enumerate(pdf.pages):
#             text = page.extract_text()
#             if not text:
#                 continue
#
#             # æ£€æµ‹æ‘˜è¦
#             if result["abstract_page"] is None and re.search(r"(æ‘˜è¦|Abstract)", text, re.IGNORECASE):
#                 result["abstract_page"] = i + 1  # é¡µç ä»1å¼€å§‹
#
#             # æ£€æµ‹ç›®å½•
#             if result["toc_page"] is None and re.search(r"(ç›®å½•|Contents)", text, re.IGNORECASE):
#                 result["toc_page"] = i + 1
#
#             # å¦‚æœéƒ½æ‰¾åˆ°äº†å°±å¯ä»¥æå‰ç»“æŸ
#             if result["abstract_page"] and result["toc_page"]:
#                 break
#     # ä¿å­˜åˆ° JSON æ–‡ä»¶
#     with open(output_json, "w", encoding="utf-8") as f:
#         json.dump(result, f, ensure_ascii=False, indent=4)
#     return result

def extract_text_from_pdf(pdf_path, pages=None):
    """
    ä» PDF æå–æ–‡æœ¬ï¼Œå¯ä»¥æŒ‡å®šé¡µç 
    :param pdf_path: PDF æ–‡ä»¶è·¯å¾„
    :param pages: None è¡¨ç¤ºå…¨éƒ¨é¡µï¼›å¯ä»¥æ˜¯ [0,2,4] è¿™æ ·çš„é¡µç åˆ—è¡¨ï¼›ä¹Ÿå¯ä»¥æ˜¯ (start, end) çš„èŒƒå›´ï¼ˆåŒ…å« startï¼Œä¸åŒ…å« endï¼‰
    :return: æå–åˆ°çš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        if pages is None:
            # æå–æ‰€æœ‰é¡µ
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        elif isinstance(pages, (list, tuple)):
            if all(isinstance(p, int) for p in pages):
                # é¡µç åˆ—è¡¨
                for p in pages:
                    if 0 <= p < len(pdf.pages):
                        page_text = pdf.pages[p].extract_text()
                        if page_text:
                            text += page_text + "\n"
            elif len(pages) == 2 and all(isinstance(p, int) for p in pages):
                # é¡µç èŒƒå›´ (start, end)
                for p in range(pages[0], pages[1]):
                    if 0 <= p < len(pdf.pages):
                        page_text = pdf.pages[p].extract_text()
                        if page_text:
                            text += page_text + "\n"
        else:
            raise ValueError("pages å‚æ•°å¿…é¡»æ˜¯ Noneã€é¡µç åˆ—è¡¨ [0,2,3] æˆ–é¡µç èŒƒå›´ (start, end)")

    return text.strip()

#æå–å›¾ç‰‡         æå–å›¾ç‰‡             æå–å›¾ç‰‡                     æå–å›¾ç‰‡
def merge_boxes_with_distance(boxes, iou_threshold=0.2, distance_threshold=15):
    merged = []
    used = [False] * len(boxes)
    for i in range(len(boxes)):
        if used[i]:
            continue
        x1, y1, w1, h1 = boxes[i]
        bx1, by1, bx2, by2 = x1, y1, x1 + w1, y1 + h1
        for j in range(i + 1, len(boxes)):
            if used[j]:
                continue
            x2, y2, w2, h2 = boxes[j]
            cx1, cy1, cx2, cy2 = x2, y2, x2 + w2, y2 + h2
            inter_x1, inter_y1 = max(bx1, cx1), max(by1, cy1)
            inter_x2, inter_y2 = min(bx2, cx2), min(by2, cy2)
            inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
            area1 = w1 * h1
            area2 = w2 * h2
            iou = inter_area / float(area1 + area2 - inter_area + 1e-6)
            horizontal_gap = max(0, max(bx1, cx1) - min(bx2, cx2))
            vertical_gap = max(0, max(by1, cy1) - min(by2, cy2))
            if iou > iou_threshold or (horizontal_gap <= distance_threshold and vertical_gap <= distance_threshold):
                bx1, by1 = min(bx1, cx1), min(by1, cy1)
                bx2, by2 = max(bx2, cx2), max(by2, cy2)
                used[j] = True
        merged.append((bx1, by1, bx2 - bx1, by2 - by1))
        used[i] = True
    return merged

def merge_text_blocks(text_blocks, scale=1.0):
    text_blocks_scaled = [
        (int(b[0]*scale), int(b[1]*scale), int(b[2]*scale), int(b[3]*scale), b[4].strip())
        for b in text_blocks if b[4].strip()
    ]
    text_blocks_scaled.sort(key=lambda b: b[1])
    merged = []
    for block in text_blocks_scaled:
        if not merged:
            merged.append(block)
        else:
            last = merged[-1]
            if block[1] <= last[3] + 5:
                mx0 = min(last[0], block[0])
                my0 = min(last[1], block[1])
                mx1 = max(last[2], block[2])
                my1 = max(last[3], block[3])
                mtext = last[4] + " " + block[4]
                merged[-1] = (mx0, my0, mx1, my1, mtext)
            else:
                merged.append(block)
    return merged

def find_caption_for_box_recursive(bbox, caption_boxes, initial_gap_factor=1.5, max_gap_factor=5.0, step=1.5):
    x1, y1, w, h = bbox
    bx1, by1, bx2, by2 = x1, y1, x1 + w, y1 + h
    caption_pattern = re.compile(r'^(å›¾|è¡¨)\s*\d+[-\d]*.*', re.IGNORECASE)
    gap_factor = initial_gap_factor
    matched = []
    while gap_factor <= max_gap_factor:
        max_gap = int(gap_factor * h)
        candidates = []
        for (tx0, ty0, tx1, ty1, text) in caption_boxes:
            if tx1 < bx1 or tx0 > bx2:
                continue
            if 0 < ty0 - by2 <= max_gap and caption_pattern.match(text):
                candidates.append((ty0 - by2, (tx0, ty0, tx1, ty1, text)))
            elif 0 < by1 - ty1 <= max_gap and caption_pattern.match(text):
                candidates.append((by1 - ty1, (tx0, ty0, tx1, ty1, text)))
        if candidates:
            candidates.sort(key=lambda x: x[0])
            min_gap = candidates[0][0]
            matched = [c[1] for c in candidates if c[0] <= min_gap + 5]
            break
        else:
            gap_factor += step
    return matched

def extract_figures_with_titles(pdf_path, output_folder, json_output,
                                dpi=200, min_area=5000, max_area_ratio=0.4, max_gap_factor=1.5):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    pdf_doc = fitz.open(pdf_path)
    results = []
    prev_page_bottom_captions = []

    for page_index in range(len(pdf_doc)):
        page = pdf_doc[page_index]
        pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))
        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        boxes = [cv2.boundingRect(cnt) for cnt in contours if cv2.contourArea(cnt) > min_area]
        merged_boxes = merge_boxes_with_distance(boxes, iou_threshold=0.2, distance_threshold=15)
        page_area = img.shape[0] * img.shape[1]
        filtered_boxes = [(x, y, w, h) for (x, y, w, h) in merged_boxes if w * h < max_area_ratio * page_area]

        scale = dpi / 72.0
        text_blocks = page.get_text("blocks")
        caption_boxes = merge_text_blocks(text_blocks, scale)
        caption_boxes_extended = prev_page_bottom_captions + caption_boxes

        final_boxes = []
        for (x, y, w, h) in filtered_boxes:
            bx1, bx2 = 0, img.shape[1]
            by1, by2 = y, y + h
            matched_caption = find_caption_for_box_recursive((bx1, by1, bx2 - bx1, by2 - by1),
                                                             caption_boxes_extended,
                                                             initial_gap_factor=max_gap_factor)
            caption_texts = []
            if matched_caption:
                ty0 = min(c[1] for c in matched_caption)
                ty1 = max(c[3] for c in matched_caption)
                caption_texts = [c[4] for c in matched_caption]
                by1 = min(by1, ty0)
                by2 = max(by2, ty1)
            final_boxes.append((bx1, by1, bx2 - bx1, by2 - by1, " ".join(caption_texts)))

        prev_page_bottom_captions = [c for c in caption_boxes if c[1] > img.shape[0]*0.7]

        img_count = 0
        for (x, y, w, h, caption_text) in final_boxes:
            roi = img[y:y + h, x:x + w]
            if roi.size == 0:
                continue
            img_filename = f"page_{page_index + 1}_fig_{img_count + 1}.png"
            abs_img_path = os.path.join(output_folder, img_filename)
            cv2.imwrite(abs_img_path, cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))

            # âœ… ä½¿ç”¨ç”¨æˆ·æŒ‡å®š output_folder + æ–‡ä»¶åï¼Œç¡®ä¿ /root/... ç»å¯¹è·¯å¾„
            abs_img_path = os.path.join("pictures_final/", img_filename).replace("\\", "/")

            fig_id, chapter, pure_caption = "", "", caption_text
            m = re.match(r'^(å›¾|è¡¨)\s*(\d+)(?:-(\d+))?\s*(.*)', caption_text)
            if m:
                prefix, ch, sec, rest = m.groups()
                if sec:
                    fig_id = f"Fig{ch}-{sec}"
                else:
                    fig_id = f"Fig{ch}"
                chapter = f"ç¬¬{ch}ç« "
                pure_caption = rest.strip()

            results.append({
                "id": fig_id,
                "caption": pure_caption,
                "path": abs_img_path,  # Linuxç»å¯¹è·¯å¾„
                "chapter": chapter
            })
            img_count += 1

    pdf_doc.close()

    with open(json_output, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("æå–å®Œæˆï¼ˆå›¾ç‰‡ + æ ‡é¢˜ + JSON ä¿å­˜ï¼‰ï¼")


#ç»Ÿè®¡æ–‡æœ¬å­—æ•°
def count_words(text):
    # åŒ¹é…ä¸­æ–‡å­—ç¬¦ï¼ˆä¸€ä¸ªæ±‰å­—è®°ä¸ºä¸€ä¸ªè¯ï¼‰
    chinese = re.findall(r'[\u4e00-\u9fff]', text)

    # åŒ¹é…è‹±æ–‡å•è¯ï¼ˆè¿ç»­å­—æ¯ç»„æˆä¸€ä¸ªå•è¯ï¼‰
    english = re.findall(r'[a-zA-Z]+', text)

    # åŒ¹é…æ•°å­—ï¼ˆå¦‚æœéœ€è¦ä¹Ÿç®—ä½œä¸€ä¸ªâ€œè¯â€ï¼‰
    numbers = re.findall(r'\d+', text)

    return len(chinese) + len(english) + len(numbers)

#æå–å­—ç¬¦ä¸²å†…æ•°å­—
def extract_number(s: str) -> int:
    match = re.search(r"\d+", s)   # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿ç»­æ•°å­—
    if match:
        return int(match.group())  # è¿”å›æ•´æ•°
    return None

#æ‹¼æ¥mdæ–‡ä»¶
def merge_md_files(files, output_file="merged.md"):
    merged = ""
    for f in files:
        merged += Path(f).read_text(encoding="utf-8").strip() + "\n\n"
    Path(output_file).write_text(merged.strip(), encoding="utf-8")
    print(f"å·²åˆå¹¶ {len(files)} ä¸ªæ–‡ä»¶ -> {output_file}")
